---
title: "Is the mushroom good to eat?"
author: "Simon Lee"
date: "UCSB Fall 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Mushroom Classification
For the project, I was looking through data from kaggle and stumbled across this mushroom dataset. What got me interested is the fact that all of the predictive variables that will be used to predict the edibility of a mushroom are categorical. With my previous experience before taking this class, it seemed to be a challenge to predict an outcome on only categorical variables. Since I've only worked with mostly logistic regression in my previous experience. So that's why I wanted to use what I've learned in this class to build an effective model with the mushroom dataset.

## Introduction
The goal of this project is to build different models that predict whether or not a certain certain mushroom is edible or
poisonous. 
```{r include = FALSE}
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(corrr)
library(corrplot)
library(klaR)
library(glmnet)
library(MASS)
library(discrim)
library(poissonreg)
library(janitor)
library(rpart.plot)
library(vip)
library(randomForest)
library(xgboost)
library(recipes)
library(dplyr)
library(ranger)
tidymodels_prefer()
```


```{r}
# reading in the data
rawData <- read.csv("mushrooms.csv")
dim(rawData)
```
There is 8124 observations and 23 variables for each of the observations.

Lets also check how many levels for each categorical variable we have. So we would have a better idea when we turn them into
factors the unique values given to each variable

```{r}
# map_df function from dplyr package in order to make each variable a factor. 
mushroom <- rawData %>% map_df(function(.x) as.factor(.x))

# The function is used to calculate the amount of levels(or categories) each variable has
levelCount <- function(x){
  x <- length(levels(x))
}

x <- mushroom %>%  map_dbl(function(.x) levelCount(.x))  %>%  as_tibble()  %>%  
       rownames_to_column()  %>%  arrange(desc(value))
colnames(x) <- c("Variable #", "Number of levels")
print(x)
```
As we can see with all the different variables, the amount of levels within each variable ranges
from 1-12. Noticing that variable 17, (veil_type) has only one level, which means that all of the
observations have the same value for that variable. With only one factor, it does not help us in determining
the edibility of a mushroom and will be removed from the data.

```{r echo=FALSE}
mushroom <- mushroom %>% select(- veil.type)
```

At this point, it might also be good to check whether or not our data has missing values
```{r echo=FALSE}
colSums(is.na(mushroom)) / nrow(mushroom)
```

Luckily for us. This data set found on kaggle does not have any missing values.

Let's take a look at the distribution of edible mushroom to poisonous in our dataset. (Observing distribution of our outcome variable)
```{r echo=FALSE}
mushroom %>% ggplot(aes(x= class)) + geom_bar()
```

The ratio of poisnous to edible mushrooms in the dataset is around the same. With there being slightly more
edible mushrooms than poisonous ones. That's good for our purpose since it wouldn't be well for the model to have an outcome variable that's skewed. Then that would make our model to always predict one outcome just because a majority of the raw data is that one outcome.

## Data Cleaning

Before visualizing some of the data with ggplot, let's make the data prettier looking. And by that I mean
changing the variable names and level names to make them easier to understand.

```{r}
colnames(mushroom) <- c("edibility", "cap_shape", "cap_surface", "cap_color", "bruises", "odor", "gill_attachment",
                        "gill_spacing", "gill_size", "gill_color", "stalk_shape", "stalk_root", "stalk_surface_above_ring",
                        "stalk_surface_below_ring", "stalk_color_above_ring", "stalk_color_below_ring", "veil_color",
                        "ring_number", "ring_type", "spore_print_color", "population", "habitat")
levels(mushroom$edibility) <- c("edible", "poisonous")
levels(mushroom$cap_shape) <- c("bell", "conical", "flat", "knobbed", "sunken", "convex")
levels(mushroom$cap_surface) <- c("fibrous", "grooves", "scaly", "smooth")
levels(mushroom$cap_color) <- c("buff", "cinnamon", "red", "gray", "brown", "pink", "green", "purple", "white", "yellow")
levels(mushroom$bruises) <- c("no", "yes")
levels(mushroom$odor) <- c("almond", "creosote", "foul", "anise", "musty", "none", "pungent", "spicy", "fishy")
levels(mushroom$gill_attachment) <- c("attched", "free")
levels(mushroom$gill_spacing) <- c("close", "crowded")
levels(mushroom$gill_size) <- c("broad", "narrow")
levels(mushroom$gill_color) <- c("buff", "red", "gray", "chocolate", "black", "brown", "orange", "pink", "green",
                                 "purple", "white", "yellow")
levels(mushroom$stalk_shape) <- c("enlarging", "tapering")
levels(mushroom$stalk_root) <- c("missing", "bulbous", "club", "equal", "rooted")
levels(mushroom$stalk_surface_above_ring) <- c("fibrous", "silky", "smooth", "scaly")
levels(mushroom$stalk_surface_below_ring) <- c("fibrous", "silky", "smooth", "scaly")
levels(mushroom$stalk_color_above_ring) <- c("buff", "cinnamon", "red", "gray", "brown", "orange", "pink",
                                             "white", "yellow")
levels(mushroom$stalk_color_below_ring) <- c("buff", "cinnamon", "red", "gray", "brown", "orange", "pink",
                                             "white", "yellow")
levels(mushroom$veil_color) <- c("brown", "orange", "white", "yellow")
levels(mushroom$ring_number) <- c("none", "one", "two")
levels(mushroom$ring_type) <- c("evanescent", "flaring", "large", "none", "pendant")
levels(mushroom$spore_print_color) <- c("buff", "chocolate", "black", "brown", "orange", "green", "purple",
                                        "white", "yellow")
levels(mushroom$population) <- c("abundant", "clustered", "numerous", "scattered", "several", "solitary")
levels(mushroom$habitat) <- c("wood", "grasses", "leaves", "meadows", "paths", "urban", "waste")
```


## EDA

Now lets use ggplot to see a general idea of which variables might be useful in predicting the edibility of a mushroom

The variables that I'm interested in visualizing is all of the color variables in the data set. This is due to the general assumption that as a survival tactic, you shouldn't eat bright colored things in nature. The term used is aposematism where it is the trait of an organism which advertises to its predators to not attack or eat it. So I'm going to make bar charts of all the
color variables and see the ratio of edible vs poisonous mushroom within each color. That way I can kind of get a general idea
of how important each variable is to predicting the edibility of the mushroom.

```{r echo = FALSE}
ggplot(data = mushroom, aes(x = cap_color, fill = edibility)) + geom_bar()
ggplot(data = mushroom, aes(x = gill_color, fill = edibility)) + geom_bar()
ggplot(data = mushroom, aes(x = stalk_color_above_ring, fill = edibility)) + geom_bar()
ggplot(data = mushroom, aes(x = stalk_color_below_ring, fill = edibility)) + geom_bar()
ggplot(data = mushroom, aes(x = veil_color, fill = edibility)) + geom_bar()
ggplot(data = mushroom, aes(x = spore_print_color, fill = edibility)) + geom_bar()
```

Looking at the six different bar charts. It seems that the color of the cap of the mushroom doesn't contribute too much to
whether or not the mushroom is edible or not. Within each cap_color, there are edible and poisonous except for green and purple which suggests that its edible. However the count size is too small to conclude anything. More interestingly in the second bar graph, if the mushroom has a buff gill_color, then it is poisonous. stalk_color_above_ring and stalk_color_below_ring have the
same distribution which makes sense as the color of the stalk below and above the ring should be the same. From this bar chart,
notable colors include gray which indicates the mushroom is edible and brown which indicates that the mushroom is poisonous. The
veil_color does not appear to be helpful for our purpose of determining the edibility of the mushroom. spore_print_color seems
to be the most useful out of the color attributes in determining the edibility of a mushroom. (buff, black, brown, orange, purple
yellow) colored spore_print most likely indicates that the mushroom is edible. (chocolate, green, white) colored spore_print indicates that the mushroom is most likely to be poisonous.


### Let's also take a look at some correlation between our variables

In order to do that, I will be changing all the categorical variables to numeric except the response variable, edibility.
```{r}
mushroom_num <- mushroom %>% map_df(function(.x) as.numeric(.x))
mushroom_num <- mushroom_num %>% select(-edibility)
mushroom_cor <- cor(mushroom_num)
mushroom_corplot <- corrplot(mushroom_cor, order = "AOE", type = "lower", tl.cex = 0.7)
```

From the correlation plot, we can see that not many of our variables correlate with each other. This makes sense as its a data set of mostly categorical variables. Which even after being turned numeric values wouldn't make any sense in correlation. Which means that most of them don't interact with one another which will make the recipe step much easier. Though a thing to keep an
eye out for would be the variables ring_number, bruises, gill_size, and stalk_shape. ring_number is originally a numerical variable and gill_size/stalk_shape have only 2 levels so after changing them to numbers they could still be used in correlation.

Let's take a look at the interaction between these 3 variables
```{r echo = FALSE}
mushroom_cor1 <- mushroom_num %>% 
  select(c(ring_number, stalk_shape, gill_size)) %>% 
  correlate() %>% 
  stretch() %>% 
  ggplot(aes(x,y,fill = r)) + geom_tile() + geom_text(aes(label = as.character(fashion(r))))
mushroom_cor1
```

There is some very weak positive/negative correlation between these variables. Which could lead me to assume
that there is no interaction between these variables.

## Model Making

It's time to split our data into the training/testing set. Create a recipe. And build different models to predict
whether or not a mushroom is edible based on its attributes

### Splitting the Data
```{r}
# seed is used to make sure that we can reproduce our data
set.seed(115)

mush_df <- rawData  %>% 
  mutate_if(is.character, as.factor) %>% 
  select(-c(bruises, gill.attachment, veil.type))


# Splitting our data into 80% training and 20% testing as well as using our response variable edibility as the strata
mush_split <- mush_df %>% initial_split(prop = 0.8, strata = class)
mush_train <- training(mush_split)
mush_test <- testing(mush_split)
```

Let's quickly check the dimensions of our training/testing set
```{r}
dim(mush_train)
dim(mush_test)
```

There are 6498 observation in the training data set and 1626 observations in the testing data set

## Recipe Building/K-Fold Cross Validation

Fortunately, using tidymodels means that we only have to create one recipe for use in all of our models. We will
also set up the k-fold cross validation in this step as well.
```{r}
# creating the recipe
mush_recipe <- recipe(class~., data = mush_train) %>% 
  step_novel(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
mush_recipe

# validation folds
mush_folds <- vfold_cv(mush_train, v = 10, strata = class)
mushrooms_prep <- prep(mush_recipe)
```

In the recipe, we used 19 of the available 22 available predictive variables since some of the predictive variables
wouldn't prove to help in the predicting of our outcome variable. Since all of our predictive variables are categorical,
they will be turned into dummy variables. then all the variables are standardized. Stratified cross validation is used here
with 10 folds to help with over-fitting and the imbalance in the data.


## Model Building

Time to train our models. Some of the models that we will be using in this project is log_reg, lda_mod, decision_tree,
random_forest, and boosted trees. To start off, we create a model by setting an engine and mode. For logistic regression 
and lda, we just need to set up the workflow, add the model, and then add in the recipe. Then we will fit the model
to the dataset and then collect metrics for the model. roc_auc will be used to measure the model performances.
```{r echo= FALSE}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_recipe(mush_recipe) %>% 
  add_model(log_reg)

lda_mod <- discrim_linear() %>% 
  set_engine("MASS") %>% 
  set_mode("classification")

lda_wkflow <- workflow() %>% 
  add_recipe(mush_recipe) %>% 
  add_model(lda_mod)
```


```{r include= FALSE}
# not included in html but this is where the sample is fitted.
log_fit <- fit_resamples(log_wkflow, mush_folds)
lda_fit <- fit_resamples(resamples = mush_folds, lda_wkflow)
```


```{r}
log_me <- collect_metrics(log_fit)
lda_me <- collect_metrics(lda_fit)
log_me
lda_me
```
Seems like both our logistic regression model and Lda model did very well on our training data. With logistic regression getting
a perfect roc_auc score and lda getting very close to one as well. 

Let's run a lda test on our testing data to see how it does...
```{r}
lda_fit_train <- fit(log_wkflow, mush_train)
lda_test <- fit(lda_wkflow, mush_test)

predict(lda_test, new_data = mush_test, type = "class") %>% 
  bind_cols(mush_test %>% select(class)) %>% 
  accuracy(truth = class, estimate = .pred_class)
```
As expected, it worked very well on the testing data as well with an accuracy of 99%.

## Slightly more complicated models

We will now make a decision tree model, a random forest model, and a boosted tree model. Just like our two previous models log_reg and lda, we will be using the same recipe. The difference here being that in these more complicated models, we need to set up a tuning grid with the parameters that we want tuned as well as the different levels we want to tune as well. Then we need to tune our models with the grid.

### Decision Tree
Let's start with making a decision tree. 
```{r}
mush_spec <- decision_tree(cost_complexity = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("rpart")

mush_tree_wf <- workflow() %>% 
  add_recipe(mush_recipe) %>% 
  add_model(mush_spec)

param_grid_tree <- grid_regular(cost_complexity(range = c(-3,-1)), levels = 10)
```


```{r eval= FALSE}
tune_res_tree <- tune_grid(mush_tree_wf, resamples = mush_folds, grid = param_grid_tree, metrics = metric_set(roc_auc))
```

```{r echo = FALSE}
load("tune_res_tree.rda")
autoplot(tune_res_tree)
```

As the cost-complexity increases the overall roc_auc goes down

```{r}
collect_metrics(tune_res_tree) %>% arrange(desc(mean))
best_parameter_tree <- select_best(tune_res_tree, metric = "roc_auc")
best_parameter_tree
```

The best performing decision tree was 0.9996 and it had a cost complexity of 0.001. So let's store that as
best_parameter_tree.

Now let's visualize the decision tree based on our best performing decision tree.
```{r echo=FALSE}
mush_tree_final <- finalize_workflow(mush_tree_wf, best_parameter_tree)
mush_tree_fit <- fit(mush_tree_final, data = mush_train)
mush_tree_fit %>%  extract_fit_engine %>% rpart.plot(roundint = FALSE)
```

If we look towards the bottom of the decision tree, it seems that the model is able to pretty much accurately able to predict
the edibility of a mushroom after not many decisions through the predictive variables. And we can get a general idea of which
predictive variables might be more significant in predicting our outcome variable. A few that keeps popping up is odor,
spore_print_color, stalk_root, gill_size...


### Random Forest

probably the model that took the longest to build. We will be making a random forest model. A random forest model chooses mtry
number of predictors that would be randomly sampled to give to the trees to make decisions. mtry is the number of randomly selected variables each tree is given. trees represents the number of trees in the forest. min_n is the minimum # of data points in each node that are required for further splitting
```{r}
mush_rf_spec <- rand_forest(mtry= tune(), trees = tune(), min_n = tune()) %>% 
  set_engine("randomForest", importance = TRUE) %>% 
  set_mode("classification")

mush_rf_wf <- workflow() %>% 
  add_recipe(mush_recipe) %>% 
  add_model(mush_rf_spec)
```

We will be using mtry from 1-19 since we have 19 predictors, otherwise we would be using predictors that aren't available
```{r}
param_grid_rf <- grid_regular(mtry(range = c(1,19)),
                              trees(range = c(1,10)),
                              min_n(range = c(1,10)),
                              levels = 8)
param_grid_rf
```

```{r eval=FALSE}
tune_res_rf <- tune_grid(mush_rf_wf, resamples = mush_folds, grid = param_grid_rf, metrics = metric_set(roc_auc))
```


```{r echo=FALSE}
load("tune_res_rf.rda")
autoplot(tune_res_rf)
```

Through all the random forest models, they all tend to do way better with more predictors. And the best performing models
tend to have more trees as well. Increasing the number of trees and randomly selected predictors increases our auc_auc score.

```{r}
collect_metrics(tune_res_rf) %>% arrange(desc(mean))
best_parameter_rf <- select_best(tune_res_rf, metric = "roc_auc")
best_parameter_rf
```

Since many of our random forest models had a mean of 1 and a std.err of 0, I just let the computer decide the best model and that
turned out to be the model with (mtry = 18, trees = 10, and min_n = 1)

```{r}
mush_rf_final <- finalize_workflow(mush_rf_wf, best_parameter_rf)
mush_rf_final_fit <- fit(mush_rf_final, data = mush_train)
```

```{r echo = FALSE}
mush_rf_final_fit %>% extract_fit_engine() %>% vip()
```

With that we can also take a look at which variables contributed the most in predicting our outcome variable. Seems that like we have seen from the decision tree model from earlier, odor, spore_print_color, and gill_size are all important predictors.

### Boosted Tree

We will now train a boosted tree model with trees ranging from 10-2000 and 10 levels.
```{r}
mush_boosted_spec <- boost_tree(trees = tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

mush_boosted_wf <- workflow() %>% 
  add_recipe(mush_recipe) %>% 
  add_model(mush_boosted_spec)

param_grid_boosted <- grid_regular(trees(range = c(10,2000)), levels = 10)
```
```{r echo=FALSE}
tune_res_boosted <- tune_grid(mush_boosted_wf, resamples = mush_folds, grid = param_grid_boosted, metrics = metric_set(roc_auc))
```

```{r echo=FALSE}
load("tune_res_boosted.rda")
```


```{r echo=FALSE}
autoplot(tune_res_boosted)
```

Though the roc_score is already high with a low number of trees, it eventually reaches 1 at around 250 trees

```{r}
collect_metrics(tune_res_boosted) %>% arrange(desc(mean))
```

```{r}
best_parameter_boosted <- select_best(tune_res_boosted, metric = "roc_auc")
best_parameter_boosted
```
231 had the highest roc_auc score while the lowest score is at 10 trees


## Model Comparisons

For comparing our models, we will be using the roc_score from collect_metrics for our models. 
```{r}
tree_auc <- collect_metrics(tune_res_tree) %>% arrange(desc(mean))
forest_auc <- collect_metrics(tune_res_rf) %>% arrange(desc(mean))
boosted_auc <- collect_metrics(tune_res_boosted) %>% arrange(desc(mean))
roc_aucs <- c(tree_auc$mean[1], forest_auc$mean[1], boosted_auc$mean[1], log_me$mean[1], lda_me$mean[1])
roc_aucs
```

All of our models did very well, three of them had roc_auc scores of 1. Which means that they are perfectly able to predict
the edibility of a mushroom based on the predictive variables. So I will selected the boosted tree model to run on the test data
to see how well it does

## Predicting on Testing Data

Here we will be finalizing our workflow to be that of the boosted tree model. Then we will test the model on our testing data which is much smaller than our training data to see how it does.
```{r}
mush_final <- finalize_workflow(mush_boosted_wf, best_parameter_boosted)
mush_final_fit <- fit(mush_final, data = mush_train)
testing_roc_auc <- augment(mush_final_fit, new_data = mush_test) %>% 
  accuracy(truth = class, estimate = .pred_class)
testing_roc_auc
```

WoW. Our model is able to completely accurately predict all of the outcome variables from 1626 obervations. Let's visualize that in a roc_curve.

### ROC curve for Boosted Tree Model
```{r}
roc_curves <- augment(mush_final_fit, new_data = mush_test) %>%
  roc_curve(truth = class, estimate =.pred_e)
roc_curves %>% autoplot()
```

As expected from our finalized model with a roc_score of 1. The curve makes a perfect right angle. Now let's take a look at the confusion matrix.

### Confusion Matrix
```{r}
final_model_conf <- augment(mush_final_fit, new_data = mush_test) %>% 
  conf_mat(truth = class, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
final_model_conf
```

100% accuracy here as well. 842 of the edible mushrooms were predicted as edible and 784 of the poisonous mushrooms were predicted as poisonous.

## Something More..

Since our data is a very popular dataset from kaggle, the predictive variables are very nice and looking through the data some more, some of our predictive variables are completely accurate in predicting the edibility of a mushroom alone without all of the other variables. So what I wanted to test out is if I removed some of the more important predictive variables like odor and spore_print_color, how would that affect the roc_score of our models.

### Recipe and Model Building

In this case, I will be taking out two more variables from our recipe, odor and spore_print_color which I believe are two of the more important variables in the dataset and fit to a decision tree model
```{r}
set.seed(115)

mush_df1 <- rawData  %>% 
  mutate_if(is.character, as.factor) %>% 
  select(-c(bruises, gill.attachment, veil.type, odor, spore.print.color))


# Splitting our data into 80% training and 20% testing as well as using our response variable edibility as the strata
mush_split1 <- mush_df1 %>% initial_split(prop = 0.8, strata = class)
mush_train1 <- training(mush_split1)
mush_test1 <- testing(mush_split1)
```

```{r}
mush_recipe1 <- recipe(class~., data = mush_train1) %>% 
  step_novel(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
mush_recipe1
```

```{r echo = FALSE}
mush_tree_wf1 <- workflow() %>% 
  add_recipe(mush_recipe1) %>% 
  add_model(mush_spec)
```

```{r echo=FALSE}
tune_res_tree1 <- tune_grid(mush_tree_wf1, resamples = mush_folds, grid = param_grid_tree, metrics = metric_set(roc_auc))
```

```{r}
collect_metrics(tune_res_tree1) %>% arrange(desc(mean))
best_parameter_tree1 <- select_best(tune_res_tree1, metric = "roc_auc")
best_parameter_tree1
```

As we can see with two of the more important predictive variables removed from the recipe, the lowest roc_auc score has gone down
to 0.9182. Which speaks volumes of how important odor and spore_print_color is to predicting the edibility of the mushroom.

## Conclusion
Overall, all of our models (logistic regression, lda, decision tree, random forest, boosted tree) were able to predict the edibility of a mushroom very well. This is thanks to to the dataset from kaggle to be very nice in the way that we are able to build models that can 100% identify the edibility of a mushroom. Which is a good thing assuming that those who go shrooming can tell if a mushroom is poisonous just by a few factors. The color of its spore print and if it has a distinct odor. Overall, this was a fun project to work on and I think by working on this project, I was able to connect ideas that I've learned throughout this whole quarter and put it to use in a meaningful way. Next time instead, I would love to use a dataset of both quantitative and qualitative to predict an outcome. Or maybe a dataset that isn't as nicely correlated between the predictive variables and outcome variable like this one.